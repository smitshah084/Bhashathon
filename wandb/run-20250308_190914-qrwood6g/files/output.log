Training Progress:   0%|‚ñè                                                                                                                          | 2/1122 [00:22<3:30:30, 11.28s/it]
ITER:  0
                                                                                                                                                                                      
iter 0: loss 10.9738, time 11446.99ms, mfu -100.00%

==================================================
PERFORMANCE METRICS (iter 0):
Training tokens/second: 44968.06
Testing tokens/second: 0.00
Parameter update time: 306.40ms (frequency: every 61 micro steps)
Data loading time: 1.61ms per batch
Train batch processing time: 182.10ms
Test batch processing time: 0.00ms
Estimated time to train on 9.1M tokens: 0h 3m 11s
==================================================

ITER:  1
iter 1: loss 10.9725, time 10906.82ms, mfu -100.00%

==================================================
PERFORMANCE METRICS (iter 1):
Training tokens/second: 44983.88
Testing tokens/second: 0.00
Parameter update time: 156.10ms (frequency: every 61 micro steps)
Data loading time: 1.64ms per batch
Train batch processing time: 179.62ms
Test batch processing time: 0.00ms
Estimated time to train on 9.1M tokens: 0h 3m 0s
==================================================

ITER:  2
iter 2: loss 10.9498, time 11149.86ms, mfu -100.00%

==================================================
PERFORMANCE METRICS (iter 2):
Training tokens/second: 44989.62
Testing tokens/second: 0.00
Parameter update time: 105.99ms (frequency: every 61 micro steps)
Data loading time: 1.60ms per batch
Train batch processing time: 179.57ms
Test batch processing time: 0.00ms
Estimated time to train on 9.1M tokens: 0h 2m 48s
==================================================

ITER:  3
iter 3: loss 10.9311, time 11148.12ms, mfu -100.00%

==================================================
PERFORMANCE METRICS (iter 3):
Training tokens/second: 44994.27
Testing tokens/second: 0.00
Parameter update time: 80.95ms (frequency: every 61 micro steps)
Data loading time: 1.59ms per batch
Train batch processing time: 179.53ms
Test batch processing time: 0.00ms
Estimated time to train on 9.1M tokens: 0h 2m 37s
==================================================

ITER:  4
iter 4: loss 10.8763, time 11147.31ms, mfu -100.00%

==================================================
PERFORMANCE METRICS (iter 4):
Training tokens/second: 44991.08
Testing tokens/second: 0.00
Parameter update time: 65.92ms (frequency: every 61 micro steps)
Data loading time: 1.58ms per batch
Train batch processing time: 179.53ms
Test batch processing time: 0.00ms
Estimated time to train on 9.1M tokens: 0h 2m 26s
==================================================

ITER:  5
iter 5: loss 10.8291, time 11147.89ms, mfu 12.28%

==================================================
PERFORMANCE METRICS (iter 5):
Training tokens/second: 44987.99
Testing tokens/second: 0.00
Parameter update time: 55.88ms (frequency: every 61 micro steps)
Data loading time: 1.61ms per batch
Train batch processing time: 179.55ms
Test batch processing time: 0.00ms
Estimated time to train on 9.1M tokens: 0h 2m 15s
==================================================

ITER:  6
iter 6: loss 10.7833, time 11147.79ms, mfu 12.28%

==================================================
PERFORMANCE METRICS (iter 6):
Training tokens/second: 44990.73
Testing tokens/second: 0.00
Parameter update time: 48.73ms (frequency: every 61 micro steps)
Data loading time: 1.60ms per batch
Train batch processing time: 179.54ms
Test batch processing time: 0.00ms
Estimated time to train on 9.1M tokens: 0h 2m 4s
==================================================

ITER:  7
iter 7: loss 10.7262, time 11148.74ms, mfu 12.28%

==================================================
PERFORMANCE METRICS (iter 7):
Training tokens/second: 45001.95
Testing tokens/second: 0.00
Parameter update time: 43.36ms (frequency: every 61 micro steps)
Data loading time: 1.55ms per batch
Train batch processing time: 179.55ms
Test batch processing time: 0.00ms
Estimated time to train on 9.1M tokens: 0h 1m 53s
==================================================

ITER:  8
iter 8: loss 10.6520, time 11148.23ms, mfu 12.28%

==================================================
PERFORMANCE METRICS (iter 8):
Training tokens/second: 45002.79
Testing tokens/second: 0.00
Parameter update time: 39.18ms (frequency: every 61 micro steps)
Data loading time: 1.53ms per batch
Train batch processing time: 179.57ms
Test batch processing time: 0.00ms
Estimated time to train on 9.1M tokens: 0h 1m 42s
==================================================

ITER:  9
iter 9: loss 10.6009, time 11149.37ms, mfu 12.28%

==================================================
PERFORMANCE METRICS (iter 9):
Training tokens/second: 45002.56
Testing tokens/second: 0.00
Parameter update time: 35.84ms (frequency: every 61 micro steps)
Data loading time: 1.55ms per batch
Train batch processing time: 179.59ms
Test batch processing time: 0.00ms
Estimated time to train on 9.1M tokens: 0h 1m 31s
==================================================

ITER:  10
iter 10: loss 10.5799, time 11148.06ms, mfu 12.28%

==================================================
PERFORMANCE METRICS (iter 10):
Training tokens/second: 45000.79
Testing tokens/second: 0.00
Parameter update time: 33.10ms (frequency: every 61 micro steps)
Data loading time: 1.54ms per batch
Train batch processing time: 179.58ms
Test batch processing time: 0.00ms
Estimated time to train on 9.1M tokens: 0h 1m 20s
==================================================

ITER:  11
iter 11: loss 10.5621, time 11148.75ms, mfu 12.28%

==================================================
PERFORMANCE METRICS (iter 11):
Training tokens/second: 44998.97
Testing tokens/second: 0.00
Parameter update time: 30.82ms (frequency: every 61 micro steps)
Data loading time: 1.54ms per batch
Train batch processing time: 179.56ms
Test batch processing time: 0.00ms
Estimated time to train on 9.1M tokens: 0h 1m 8s
==================================================

ITER:  12
iter 12: loss 10.4912, time 11147.66ms, mfu 12.28%

==================================================
PERFORMANCE METRICS (iter 12):
Training tokens/second: 44999.30
Testing tokens/second: 0.00
Parameter update time: 28.89ms (frequency: every 61 micro steps)
Data loading time: 1.56ms per batch
Train batch processing time: 179.53ms
Test batch processing time: 0.00ms
Estimated time to train on 9.1M tokens: 0h 0m 57s
==================================================

ITER:  13
iter 13: loss 10.4637, time 11148.48ms, mfu 12.28%

==================================================
PERFORMANCE METRICS (iter 13):
Training tokens/second: 45000.10
Testing tokens/second: 0.00
Parameter update time: 27.24ms (frequency: every 61 micro steps)
Data loading time: 1.56ms per batch
Train batch processing time: 179.55ms
Test batch processing time: 0.00ms
Estimated time to train on 9.1M tokens: 0h 0m 46s
==================================================

ITER:  14
iter 14: loss 10.4668, time 11148.13ms, mfu 12.28%

==================================================
PERFORMANCE METRICS (iter 14):
Training tokens/second: 44999.98
Testing tokens/second: 0.00
Parameter update time: 25.81ms (frequency: every 61 micro steps)
Data loading time: 1.59ms per batch
Train batch processing time: 179.58ms
Test batch processing time: 0.00ms
Estimated time to train on 9.1M tokens: 0h 0m 35s
==================================================

ITER:  15
saving checkpoint to out
  File "/data/Bhashathon/train.py", line 565, in <module>
    torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))
  File "/data/miniconda/envs/aws_env/lib/python3.12/site-packages/torch/serialization.py", line 850, in save
    _save(
  File "/data/miniconda/envs/aws_env/lib/python3.12/site-packages/torch/serialization.py", line 1114, in _save
    zip_file.write_record(name, storage, num_bytes)
KeyboardInterrupt
